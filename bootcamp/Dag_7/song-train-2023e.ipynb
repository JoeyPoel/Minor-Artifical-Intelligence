{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, GRU , Bidirectional, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my\n",
      "At Waterloo Napoleon did surrender\n",
      "Oh yeah\n",
      "And I have met my destiny in quite a similar way\n",
      "The history book on the shelf\n",
      "Is always repeating itself\n",
      "Waterloo I was defeated, you won the war\n",
      "Waterloo promise to love you for ever more\n",
      "Waterloo couldn't escape if I wanted to\n",
      "Waterloo knowing my fate is to be with you\n",
      "Waterloo finally facing my Waterloo\n",
      "My my\n",
      "I tried to hold you back, but you were stronger\n",
      "Oh yeah\n",
      "And now it seems my only chance is giving up the fight\n",
      "And how co\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('abba.csv')\n",
    "\n",
    "# Concatenate all lyrics into a single string\n",
    "corpus = ' '.join(data['lyrics'].dropna())\n",
    "\n",
    "# Print the first 500 characters of the corpus just to verify\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to 'cleaned_corpus.txt'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove Unicode characters\n",
    "    cleaned_text = re.sub('[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove \\r and other special characters\n",
    "    cleaned_text = re.sub(r'[\\r\\n\\t]', ' ', cleaned_text)\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Clean the corpus\n",
    "corpus = clean_text(corpus)\n",
    "\n",
    "# Save the cleaned corpus to a file\n",
    "with open('cleaned_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(\"Data cleaned and saved to 'cleaned_corpus.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder dictionary:\n",
      "{' ': 0, '!': 1, '\"': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '6': 15, ':': 16, '?': 17, 'A': 18, 'B': 19, 'C': 20, 'D': 21, 'E': 22, 'F': 23, 'G': 24, 'H': 25, 'I': 26, 'J': 27, 'K': 28, 'L': 29, 'M': 30, 'N': 31, 'O': 32, 'P': 33, 'Q': 34, 'R': 35, 'S': 36, 'T': 37, 'U': 38, 'V': 39, 'W': 40, 'Y': 41, 'Z': 42, '[': 43, ']': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'w': 67, 'x': 68, 'y': 69, 'z': 70, '\\x7f': 71}\n",
      "\n",
      "Decoder dictionary:\n",
      "{0: ' ', 1: '!', 2: '\"', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '/', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '6', 16: ':', 17: '?', 18: 'A', 19: 'B', 20: 'C', 21: 'D', 22: 'E', 23: 'F', 24: 'G', 25: 'H', 26: 'I', 27: 'J', 28: 'K', 29: 'L', 30: 'M', 31: 'N', 32: 'O', 33: 'P', 34: 'Q', 35: 'R', 36: 'S', 37: 'T', 38: 'U', 39: 'V', 40: 'W', 41: 'Y', 42: 'Z', 43: '[', 44: ']', 45: 'a', 46: 'b', 47: 'c', 48: 'd', 49: 'e', 50: 'f', 51: 'g', 52: 'h', 53: 'i', 54: 'j', 55: 'k', 56: 'l', 57: 'm', 58: 'n', 59: 'o', 60: 'p', 61: 'q', 62: 'r', 63: 's', 64: 't', 65: 'u', 66: 'v', 67: 'w', 68: 'x', 69: 'y', 70: 'z', 71: '\\x7f'}\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters from the corpus\n",
    "unique_chars = sorted(set(corpus))\n",
    "\n",
    "# Create encoder dictionary (character to integer)\n",
    "encoder_dict = {char: i for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Create decoder dictionary (integer to character)\n",
    "decoder_dict = {i: char for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Print encoder dictionary\n",
    "print(\"Encoder dictionary:\")\n",
    "print(encoder_dict)\n",
    "\n",
    "# Print decoder dictionary\n",
    "print(\"\\nDecoder dictionary:\")\n",
    "print(decoder_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the corpus into semi-redundant sequences of 20 characters, and encode them using our dictionaries.\n",
    "# Print the first 20 characters of the corpus just to verify\n",
    "# sliced_corpus = cleaned_corpus[:20]\n",
    "# encoder_dict(sliced_corpus)\n",
    "\n",
    "sentence_length = 20\n",
    "\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoder_dict[char] for char in sentence])\n",
    "    y_data.append(encoder_dict[next_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([69, 0, 57, 69, 0, 18, 64, 0, 40, 45, 64, 49, 62, 56, 59, 59, 0, 31, 45, 60],\n",
       " 59)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple check.\n",
    "X_data[1], y_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 210090 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing X and y...\n"
     ]
    }
   ],
   "source": [
    "# now we need one-hot encoding\n",
    "num_chars = len(unique_chars)\n",
    "\n",
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's build model.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 20, 32)            13440     \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 20, 64)            16640     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 20, 64)            0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 72)                4680      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 72)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67784 (264.78 KB)\n",
      "Trainable params: 67784 (264.78 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define our model\n",
    "print(\"Let's build model.\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(sentence_length, num_chars), return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))  \n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model! save the weights after each epoch if it's a new best for loss and save model architecture.\n",
    "architecture = model.to_json()\n",
    "with open('model.json', 'w') as model_file:\n",
    "    model_file.write(architecture)\n",
    "\n",
    "# Set up checkpoints, and save trained model\n",
    "file_path=\"weights-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 2.7706\n",
      "Epoch 1: loss improved from inf to 2.77055, saving model to weights-01.hdf5\n",
      "1642/1642 [==============================] - 73s 39ms/step - loss: 2.7706\n",
      "Epoch 2/20\n",
      "   3/1642 [..............................] - ETA: 1:06 - loss: 2.5550"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1641/1642 [============================>.] - ETA: 0s - loss: 2.3897\n",
      "Epoch 2: loss improved from 2.77055 to 2.38970, saving model to weights-02.hdf5\n",
      "1642/1642 [==============================] - 63s 38ms/step - loss: 2.3897\n",
      "Epoch 3/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 2.2376\n",
      "Epoch 3: loss improved from 2.38970 to 2.23750, saving model to weights-03.hdf5\n",
      "1642/1642 [==============================] - 61s 37ms/step - loss: 2.2375\n",
      "Epoch 4/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 2.1194\n",
      "Epoch 4: loss improved from 2.23750 to 2.11943, saving model to weights-04.hdf5\n",
      "1642/1642 [==============================] - 77s 47ms/step - loss: 2.1194\n",
      "Epoch 5/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 2.0295\n",
      "Epoch 5: loss improved from 2.11943 to 2.02953, saving model to weights-05.hdf5\n",
      "1642/1642 [==============================] - 71s 43ms/step - loss: 2.0295\n",
      "Epoch 6/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.9568\n",
      "Epoch 6: loss improved from 2.02953 to 1.95677, saving model to weights-06.hdf5\n",
      "1642/1642 [==============================] - 71s 43ms/step - loss: 1.9568\n",
      "Epoch 7/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.9016\n",
      "Epoch 7: loss improved from 1.95677 to 1.90162, saving model to weights-07.hdf5\n",
      "1642/1642 [==============================] - 62s 38ms/step - loss: 1.9016\n",
      "Epoch 8/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.8530\n",
      "Epoch 8: loss improved from 1.90162 to 1.85302, saving model to weights-08.hdf5\n",
      "1642/1642 [==============================] - 61s 37ms/step - loss: 1.8530\n",
      "Epoch 9/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.8125\n",
      "Epoch 9: loss improved from 1.85302 to 1.81254, saving model to weights-09.hdf5\n",
      "1642/1642 [==============================] - 64s 39ms/step - loss: 1.8125\n",
      "Epoch 10/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.7734\n",
      "Epoch 10: loss improved from 1.81254 to 1.77350, saving model to weights-10.hdf5\n",
      "1642/1642 [==============================] - 71s 43ms/step - loss: 1.7735\n",
      "Epoch 11/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.7410\n",
      "Epoch 11: loss improved from 1.77350 to 1.74103, saving model to weights-11.hdf5\n",
      "1642/1642 [==============================] - 65s 40ms/step - loss: 1.7410\n",
      "Epoch 12/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.7116\n",
      "Epoch 12: loss improved from 1.74103 to 1.71165, saving model to weights-12.hdf5\n",
      "1642/1642 [==============================] - 72s 44ms/step - loss: 1.7116\n",
      "Epoch 13/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.6855\n",
      "Epoch 13: loss improved from 1.71165 to 1.68546, saving model to weights-13.hdf5\n",
      "1642/1642 [==============================] - 69s 42ms/step - loss: 1.6855\n",
      "Epoch 14/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.6775\n",
      "Epoch 14: loss improved from 1.68546 to 1.67753, saving model to weights-14.hdf5\n",
      "1642/1642 [==============================] - 69s 42ms/step - loss: 1.6775\n",
      "Epoch 15/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.6403\n",
      "Epoch 15: loss improved from 1.67753 to 1.64032, saving model to weights-15.hdf5\n",
      "1642/1642 [==============================] - 64s 39ms/step - loss: 1.6403\n",
      "Epoch 16/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.6178\n",
      "Epoch 16: loss improved from 1.64032 to 1.61782, saving model to weights-16.hdf5\n",
      "1642/1642 [==============================] - 64s 39ms/step - loss: 1.6178\n",
      "Epoch 17/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.6014\n",
      "Epoch 17: loss improved from 1.61782 to 1.60137, saving model to weights-17.hdf5\n",
      "1642/1642 [==============================] - 63s 38ms/step - loss: 1.6014\n",
      "Epoch 18/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.5860\n",
      "Epoch 18: loss improved from 1.60137 to 1.58597, saving model to weights-18.hdf5\n",
      "1642/1642 [==============================] - 62s 38ms/step - loss: 1.5860\n",
      "Epoch 19/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.5722\n",
      "Epoch 19: loss improved from 1.58597 to 1.57219, saving model to weights-19.hdf5\n",
      "1642/1642 [==============================] - 60s 37ms/step - loss: 1.5722\n",
      "Epoch 20/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.5591\n",
      "Epoch 20: loss improved from 1.57219 to 1.55913, saving model to weights-20.hdf5\n",
      "1642/1642 [==============================] - 61s 37ms/step - loss: 1.5591\n"
     ]
    }
   ],
   "source": [
    "# Train het model voor minimaal 20 epochs, En selecteer het dat weights-file die de beste resultaten geeft?\n",
    "# Is er ook een weights-file waarvanje kunt zeggen dat er overfitting optreedt?\n",
    "\n",
    "# kijk goed naar de onderstaande regel.\n",
    "\n",
    "history = model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
