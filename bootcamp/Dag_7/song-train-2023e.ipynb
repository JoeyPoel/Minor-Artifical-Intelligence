{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, GRU , Bidirectional, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my\n",
      "At Waterloo Napoleon did surrender\n",
      "Oh yeah\n",
      "And I have met my destiny in quite a similar way\n",
      "The history book on the shelf\n",
      "Is always repeating itself\n",
      "Waterloo I was defeated, you won the war\n",
      "Waterloo promise to love you for ever more\n",
      "Waterloo couldn't escape if I wanted to\n",
      "Waterloo knowing my fate is to be with you\n",
      "Waterloo finally facing my Waterloo\n",
      "My my\n",
      "I tried to hold you back, but you were stronger\n",
      "Oh yeah\n",
      "And now it seems my only chance is giving up the fight\n",
      "And how co\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('abba.csv')\n",
    "\n",
    "# Concatenate all lyrics into a single string\n",
    "corpus = ' '.join(data['lyrics'].dropna())\n",
    "\n",
    "# Print the first 500 characters of the corpus just to verify\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to 'cleaned_corpus.txt'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove Unicode characters\n",
    "    cleaned_text = re.sub('[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove \\r and other special characters\n",
    "    cleaned_text = re.sub(r'[\\r\\n\\t]', ' ', cleaned_text)\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Clean the corpus\n",
    "corpus = clean_text(corpus)\n",
    "\n",
    "# Save the cleaned corpus to a file\n",
    "with open('cleaned_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(\"Data cleaned and saved to 'cleaned_corpus.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder dictionary:\n",
      "{' ': 0, '!': 1, '\"': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '6': 15, ':': 16, '?': 17, 'A': 18, 'B': 19, 'C': 20, 'D': 21, 'E': 22, 'F': 23, 'G': 24, 'H': 25, 'I': 26, 'J': 27, 'K': 28, 'L': 29, 'M': 30, 'N': 31, 'O': 32, 'P': 33, 'Q': 34, 'R': 35, 'S': 36, 'T': 37, 'U': 38, 'V': 39, 'W': 40, 'Y': 41, 'Z': 42, '[': 43, ']': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'w': 67, 'x': 68, 'y': 69, 'z': 70, '\\x7f': 71}\n",
      "\n",
      "Decoder dictionary:\n",
      "{0: ' ', 1: '!', 2: '\"', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '/', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '6', 16: ':', 17: '?', 18: 'A', 19: 'B', 20: 'C', 21: 'D', 22: 'E', 23: 'F', 24: 'G', 25: 'H', 26: 'I', 27: 'J', 28: 'K', 29: 'L', 30: 'M', 31: 'N', 32: 'O', 33: 'P', 34: 'Q', 35: 'R', 36: 'S', 37: 'T', 38: 'U', 39: 'V', 40: 'W', 41: 'Y', 42: 'Z', 43: '[', 44: ']', 45: 'a', 46: 'b', 47: 'c', 48: 'd', 49: 'e', 50: 'f', 51: 'g', 52: 'h', 53: 'i', 54: 'j', 55: 'k', 56: 'l', 57: 'm', 58: 'n', 59: 'o', 60: 'p', 61: 'q', 62: 'r', 63: 's', 64: 't', 65: 'u', 66: 'v', 67: 'w', 68: 'x', 69: 'y', 70: 'z', 71: '\\x7f'}\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters from the corpus\n",
    "unique_chars = sorted(set(corpus))\n",
    "\n",
    "# Create encoder dictionary (character to integer)\n",
    "encoder_dict = {char: i for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Create decoder dictionary (integer to character)\n",
    "decoder_dict = {i: char for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Print encoder dictionary\n",
    "print(\"Encoder dictionary:\")\n",
    "print(encoder_dict)\n",
    "\n",
    "# Print decoder dictionary\n",
    "print(\"\\nDecoder dictionary:\")\n",
    "print(decoder_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the corpus into semi-redundant sequences of 20 characters, and encode them using our dictionaries.\n",
    "# Print the first 20 characters of the corpus just to verify\n",
    "# sliced_corpus = cleaned_corpus[:20]\n",
    "# encoder_dict(sliced_corpus)\n",
    "\n",
    "sentence_length = 20\n",
    "\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoder_dict[char] for char in sentence])\n",
    "    y_data.append(encoder_dict[next_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([69, 0, 57, 69, 0, 18, 64, 0, 40, 45, 64, 49, 62, 56, 59, 59, 0, 31, 45, 60],\n",
       " 59)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple check.\n",
    "X_data[1], y_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 210090 sentences of length 20\n"
     ]
    }
   ],
   "source": [
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing X and y...\n"
     ]
    }
   ],
   "source": [
    "# now we need one-hot encoding\n",
    "num_chars = len(unique_chars)\n",
    "\n",
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's build model.\n",
      "WARNING:tensorflow:From c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20, 32)            13440     \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 20, 64)            16640     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 64)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 72)                4680      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 72)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67784 (264.78 KB)\n",
      "Trainable params: 67784 (264.78 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define our model\n",
    "print(\"Let's build model.\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(sentence_length, num_chars), return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))  \n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model! save the weights after each epoch if it's a new best for loss and save model architecture.\n",
    "architecture = model.to_json()\n",
    "with open('model.json', 'w') as model_file:\n",
    "    model_file.write(architecture)\n",
    "\n",
    "# Set up checkpoints, and save trained model\n",
    "file_path=\"weights-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train het model voor minimaal 20 epochs, En selecteer het dat weights-file die de beste resultaten geeft?\n",
    "# Is er ook een weights-file waarvanje kunt zeggen dat er overfitting optreedt?\n",
    "\n",
    "# kijk goed naar de onderstaande regel.\n",
    "\n",
    "history = model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
