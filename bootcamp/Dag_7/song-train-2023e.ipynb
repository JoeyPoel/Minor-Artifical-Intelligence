{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, GRU , Bidirectional, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('abba.csv')\n",
    "\n",
    "# Concatenate all lyrics into a single string\n",
    "corpus = ' '.join(data['lyrics'].dropna())\n",
    "\n",
    "# Print the first 500 characters of the corpus just to verify\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove Unicode characters\n",
    "    cleaned_text = re.sub('[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove \\r and other special characters\n",
    "    cleaned_text = re.sub(r'[\\r\\n\\t]', ' ', cleaned_text)\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Clean the corpus\n",
    "corpus = clean_text(corpus)\n",
    "\n",
    "# Save the cleaned corpus to a file\n",
    "with open('cleaned_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(\"Data cleaned and saved to 'cleaned_corpus.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique characters from the corpus\n",
    "unique_chars = sorted(set(corpus))\n",
    "\n",
    "# Create encoder dictionary (character to integer)\n",
    "encoder_dict = {char: i for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Create decoder dictionary (integer to character)\n",
    "decoder_dict = {i: char for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Print encoder dictionary\n",
    "print(\"Encoder dictionary:\")\n",
    "print(encoder_dict)\n",
    "\n",
    "# Print decoder dictionary\n",
    "print(\"\\nDecoder dictionary:\")\n",
    "print(decoder_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the corpus into semi-redundant sequences of 20 characters, and encode them using our dictionaries.\n",
    "# Print the first 20 characters of the corpus just to verify\n",
    "# sliced_corpus = cleaned_corpus[:20]\n",
    "# encoder_dict(sliced_corpus)\n",
    "\n",
    "sentence_length = 20\n",
    "\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoder_dict[char] for char in sentence])\n",
    "    y_data.append(encoder_dict[next_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple check.\n",
    "X_data[1], y_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need one-hot encoding\n",
    "num_chars = len(unique_chars)\n",
    "\n",
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model\n",
    "print(\"Let's build model.\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(sentence_length, num_chars), return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))  \n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our model! save the weights after each epoch if it's a new best for loss and save model architecture.\n",
    "architecture = model.to_json()\n",
    "with open('model.json', 'w') as model_file:\n",
    "    model_file.write(architecture)\n",
    "\n",
    "# Set up checkpoints, and save trained model\n",
    "file_path=\"weights-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train het model voor minimaal 20 epochs, En selecteer het dat weights-file die de beste resultaten geeft?\n",
    "# Is er ook een weights-file waarvanje kunt zeggen dat er overfitting optreedt?\n",
    "\n",
    "# kijk goed naar de onderstaande regel.\n",
    "\n",
    "history = model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
