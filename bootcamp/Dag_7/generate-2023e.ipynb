{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import LSTM, Dense, Activation, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n",
    "from keras.models import model_from_yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my\n",
      "At Waterloo Napoleon did surrender\n",
      "Oh yeah\n",
      "And I have met my destiny in quite a similar way\n",
      "The history book on the shelf\n",
      "Is always repeating itself\n",
      "Waterloo I was defeated, you won the war\n",
      "Waterloo promise to love you for ever more\n",
      "Waterloo couldn't escape if I wanted to\n",
      "Waterloo knowing my fate is to be with you\n",
      "Waterloo finally facing my Waterloo\n",
      "My my\n",
      "I tried to hold you back, but you were stronger\n",
      "Oh yeah\n",
      "And now it seems my only chance is giving up the fight\n",
      "And how co\n"
     ]
    }
   ],
   "source": [
    "# Read in the data (copy cel from song-train).\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('abba.csv')\n",
    "\n",
    "# Concatenate all lyrics into a single string\n",
    "corpus = ' '.join(data['lyrics'].dropna())\n",
    "\n",
    "# Print the first 500 characters of the corpus just to verify\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to 'cleaned_corpus.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Clean the data. (copy cell from song-train)\n",
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove Unicode characters\n",
    "    cleaned_text = re.sub('[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove \\r and other special characters\n",
    "    cleaned_text = re.sub(r'[\\r\\n\\t]', ' ', cleaned_text)\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Clean the corpus\n",
    "corpus = clean_text(corpus)\n",
    "\n",
    "# Save the cleaned corpus to a file\n",
    "with open('cleaned_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(\"Data cleaned and saved to 'cleaned_corpus.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder dictionary:\n",
      "{' ': 0, '!': 1, '\"': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '6': 15, ':': 16, '?': 17, 'A': 18, 'B': 19, 'C': 20, 'D': 21, 'E': 22, 'F': 23, 'G': 24, 'H': 25, 'I': 26, 'J': 27, 'K': 28, 'L': 29, 'M': 30, 'N': 31, 'O': 32, 'P': 33, 'Q': 34, 'R': 35, 'S': 36, 'T': 37, 'U': 38, 'V': 39, 'W': 40, 'Y': 41, 'Z': 42, '[': 43, ']': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'w': 67, 'x': 68, 'y': 69, 'z': 70, '\\x7f': 71}\n",
      "\n",
      "Decoder dictionary:\n",
      "{0: ' ', 1: '!', 2: '\"', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '/', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '6', 16: ':', 17: '?', 18: 'A', 19: 'B', 20: 'C', 21: 'D', 22: 'E', 23: 'F', 24: 'G', 25: 'H', 26: 'I', 27: 'J', 28: 'K', 29: 'L', 30: 'M', 31: 'N', 32: 'O', 33: 'P', 34: 'Q', 35: 'R', 36: 'S', 37: 'T', 38: 'U', 39: 'V', 40: 'W', 41: 'Y', 42: 'Z', 43: '[', 44: ']', 45: 'a', 46: 'b', 47: 'c', 48: 'd', 49: 'e', 50: 'f', 51: 'g', 52: 'h', 53: 'i', 54: 'j', 55: 'k', 56: 'l', 57: 'm', 58: 'n', 59: 'o', 60: 'p', 61: 'q', 62: 'r', 63: 's', 64: 't', 65: 'u', 66: 'v', 67: 'w', 68: 'x', 69: 'y', 70: 'z', 71: '\\x7f'}\n"
     ]
    }
   ],
   "source": [
    "# Create encoder and decoder dictionaries. (copy-cel from song-train)\n",
    "# Get unique characters from the corpus\n",
    "unique_chars = sorted(set(corpus))\n",
    "\n",
    "# Create encoder dictionary (character to integer)\n",
    "encoder_dict = {char: i for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Create decoder dictionary (integer to character)\n",
    "decoder_dict = {i: char for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Print encoder dictionary\n",
    "print(\"Encoder dictionary:\")\n",
    "print(encoder_dict)\n",
    "\n",
    "# Print decoder dictionary\n",
    "print(\"\\nDecoder dictionary:\")\n",
    "print(decoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model from json file and load trained weights from hdf5 file.\n",
    "# UITZOEKEN VIA DE TENSOFLOW SITE HOE DIT MOET.\n",
    "# b.v.  https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "for i in range(19):\n",
    "    model.load_weights('./weights-' + str(i + 1).zfill(2) + '.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uitleg in de les\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uitleg in de les\n",
    "num_chars = len(unique_chars)\n",
    "corpus_length = len(corpus)\n",
    "\n",
    "def generate(seed_pattern):\n",
    "        X = np.zeros((1, sentence_length, num_chars), dtype=bool)\n",
    "        for i, character in enumerate(seed_pattern):\n",
    "            X[0, i, encoder_dict[character]] = 1\n",
    "        \n",
    "        generated_text = \"\"\n",
    "        for i in range(500):\n",
    "            # even de temperatuur toevoegen.\n",
    "            pred = model.predict(X, verbose=0)[0]\n",
    "            prediction = sample(pred, 0.3)\n",
    "\n",
    "            generated_text += decoder_dict[prediction]\n",
    "\n",
    "            activations = np.zeros((1, 1, num_chars), dtype=bool)\n",
    "            activations[0, 0, prediction] = 1\n",
    "            X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "sentence_length = 20\n",
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:\n",
    "            seed = randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "\n",
    "        return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the bard and show "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 20, 90), found shape=(None, 20, 72)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m seed \u001b[38;5;241m=\u001b[39m make_seed(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min the bard and show\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(seed, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m txt \u001b[38;5;241m=\u001b[39m  \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(txt)\n",
      "Cell \u001b[1;32mIn[84], line 13\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(seed_pattern)\u001b[0m\n\u001b[0;32m     10\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# even de temperatuur toevoegen.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m sample(pred, \u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m     16\u001b[0m     generated_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m decoder_dict[prediction]\n",
      "File \u001b[1;32mc:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_49nrkdy.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 20, 90), found shape=(None, 20, 72)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = make_seed('in the bard and show')\n",
    "print(seed, end=\" \")\n",
    "txt =  generate(seed)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample song #1\n",
    "In the bard and show you on your lovelight and i can't get the mowner i'm a marion an and every mind, there's a boot\n",
    "\n",
    "## Sample song #1\n",
    "in the bard and show i don't be a world be the song i mave the was the so the street you the the be me\n",
    "\n",
    "## Sample song 3 \n",
    "in the bard and show a stay it the fart the the me the the the all i ding you the the now the song the the the so some"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
