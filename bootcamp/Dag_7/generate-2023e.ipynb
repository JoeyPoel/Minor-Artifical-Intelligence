{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import LSTM, Dense, Activation, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n",
    "from keras.models import model_from_yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my\n",
      "At Waterloo Napoleon did surrender\n",
      "Oh yeah\n",
      "And I have met my destiny in quite a similar way\n",
      "The history book on the shelf\n",
      "Is always repeating itself\n",
      "Waterloo I was defeated, you won the war\n",
      "Waterloo promise to love you for ever more\n",
      "Waterloo couldn't escape if I wanted to\n",
      "Waterloo knowing my fate is to be with you\n",
      "Waterloo finally facing my Waterloo\n",
      "My my\n",
      "I tried to hold you back, but you were stronger\n",
      "Oh yeah\n",
      "And now it seems my only chance is giving up the fight\n",
      "And how co\n"
     ]
    }
   ],
   "source": [
    "# Read in the data (copy cel from song-train).\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('abba.csv')\n",
    "\n",
    "# Concatenate all lyrics into a single string\n",
    "corpus = ' '.join(data['lyrics'].dropna())\n",
    "\n",
    "# Print the first 500 characters of the corpus just to verify\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to 'cleaned_corpus.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Clean the data. (copy cell from song-train)\n",
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove Unicode characters\n",
    "    cleaned_text = re.sub('[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove \\r and other special characters\n",
    "    cleaned_text = re.sub(r'[\\r\\n\\t]', ' ', cleaned_text)\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Clean the corpus\n",
    "corpus = clean_text(corpus)\n",
    "\n",
    "# Save the cleaned corpus to a file\n",
    "with open('cleaned_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(\"Data cleaned and saved to 'cleaned_corpus.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder dictionary:\n",
      "{' ': 0, '!': 1, '\"': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '6': 15, ':': 16, '?': 17, 'A': 18, 'B': 19, 'C': 20, 'D': 21, 'E': 22, 'F': 23, 'G': 24, 'H': 25, 'I': 26, 'J': 27, 'K': 28, 'L': 29, 'M': 30, 'N': 31, 'O': 32, 'P': 33, 'Q': 34, 'R': 35, 'S': 36, 'T': 37, 'U': 38, 'V': 39, 'W': 40, 'Y': 41, 'Z': 42, '[': 43, ']': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'w': 67, 'x': 68, 'y': 69, 'z': 70, '\\x7f': 71}\n",
      "\n",
      "Decoder dictionary:\n",
      "{0: ' ', 1: '!', 2: '\"', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '/', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '6', 16: ':', 17: '?', 18: 'A', 19: 'B', 20: 'C', 21: 'D', 22: 'E', 23: 'F', 24: 'G', 25: 'H', 26: 'I', 27: 'J', 28: 'K', 29: 'L', 30: 'M', 31: 'N', 32: 'O', 33: 'P', 34: 'Q', 35: 'R', 36: 'S', 37: 'T', 38: 'U', 39: 'V', 40: 'W', 41: 'Y', 42: 'Z', 43: '[', 44: ']', 45: 'a', 46: 'b', 47: 'c', 48: 'd', 49: 'e', 50: 'f', 51: 'g', 52: 'h', 53: 'i', 54: 'j', 55: 'k', 56: 'l', 57: 'm', 58: 'n', 59: 'o', 60: 'p', 61: 'q', 62: 'r', 63: 's', 64: 't', 65: 'u', 66: 'v', 67: 'w', 68: 'x', 69: 'y', 70: 'z', 71: '\\x7f'}\n"
     ]
    }
   ],
   "source": [
    "# Create encoder and decoder dictionaries. (copy-cel from song-train)\n",
    "# Get unique characters from the corpus\n",
    "unique_chars = sorted(set(corpus))\n",
    "\n",
    "# Create encoder dictionary (character to integer)\n",
    "encoder_dict = {char: i for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Create decoder dictionary (integer to character)\n",
    "decoder_dict = {i: char for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Print encoder dictionary\n",
    "print(\"Encoder dictionary:\")\n",
    "print(encoder_dict)\n",
    "\n",
    "# Print decoder dictionary\n",
    "print(\"\\nDecoder dictionary:\")\n",
    "print(decoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model from json file and load trained weights from hdf5 file.\n",
    "# UITZOEKEN VIA DE TENSOFLOW SITE HOE DIT MOET.\n",
    "# b.v.  https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "for i in range(19):\n",
    "    model.load_weights('./weights-' + str(i + 1).zfill(2) + '.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uitleg in de les\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uitleg in de les\n",
    "num_chars = len(unique_chars)\n",
    "corpus_length = len(corpus)\n",
    "\n",
    "def generate(seed_pattern):\n",
    "        X = np.zeros((1, sentence_length, num_chars), dtype=bool)\n",
    "        for i, character in enumerate(seed_pattern):\n",
    "            X[0, i, encoder_dict[character]] = 1\n",
    "        \n",
    "        generated_text = \"\"\n",
    "        for i in range(500):\n",
    "            # even de temperatuur toevoegen.\n",
    "            pred = model.predict(X, verbose=0)[0]\n",
    "            prediction = sample(pred, 0.3)\n",
    "\n",
    "            generated_text += decoder_dict[prediction]\n",
    "\n",
    "            activations = np.zeros((1, 1, num_chars), dtype=bool)\n",
    "            activations[0, 0, prediction] = 1\n",
    "            X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "sentence_length = 20\n",
    "def make_seed(seed_phrase=\"\"):\n",
    "        if seed_phrase:\n",
    "            phrase_length = len(seed_phrase)\n",
    "            pattern = \"\"\n",
    "            for i in range (0, sentence_length):\n",
    "                pattern += seed_phrase[i % phrase_length]\n",
    "        else:\n",
    "            seed = randint(0, corpus_length - sentence_length)\n",
    "            pattern = corpus[seed:seed + sentence_length]\n",
    "\n",
    "        return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the bard and show  I can do I want that you're a light I see that you can the time is the same is all be and the pay We're think to say and say and the sare of of a mide I for the same of the dark on the mime I was a man And the same it still say the bean I can do and the seest of the and I was to the same of me with no more me we have to be with you I me I was that I have to say a bean I was strough the darking on the more The same of the midnight the supar I was the time is all the sare I was to the same it was\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = make_seed('In the bard and show you on your lovelight and i can\\'t get the mowner i\\'m a marion an and every mind, there\\'s a boot')\n",
    "print(seed, end=\" \")\n",
    "txt =  generate(seed)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample song #1\n",
    "In the bard and show you on your lovelight and i can't get the mowner i'm a marion an and every mind, there's a boot\n",
    "\n",
    "## Sample song #1\n",
    "in the bard and show i don't be a world be the song i mave the was the so the street you the the be me\n",
    "\n",
    "## Sample song 3 \n",
    "in the bard and show a stay it the fart the the me the the the all i ding you the the now the song the the the so some"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
