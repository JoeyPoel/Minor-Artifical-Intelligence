{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM\n",
    "\n",
    "This notebook demonstrates how to train a Long Short-Term Memory (LSTM) neural network for text generation using song lyrics data. We'll start by preprocessing the data, defining the model architecture, training the model, and finally generating text using the trained model.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll follow these steps:\n",
    "\n",
    "1. Data Preparation: Read song lyrics data from a CSV file, clean the text data, and prepare it for model training.\n",
    "2. Model Architecture Definition: Define an LSTM-based neural network architecture for text generation using Keras.\n",
    "3. Model Training: Train the defined model on the prepared data.\n",
    "4. Text Generation: Generate text using the trained model.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Dropout, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We'll start by reading the song lyrics data from a CSV file and cleaning the text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('abba.csv')\n",
    "\n",
    "# Concatenate all lyrics into a single string\n",
    "corpus = ' '.join(data['lyrics'].dropna())\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[^\\x00-\\x7F]+', '', text)\n",
    "    cleaned_text = re.sub(r'[\\r\\n\\t]', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "corpus = clean_text(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's proceed to encode the characters and slice the corpus into sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique characters from the corpus\n",
    "unique_chars = sorted(set(corpus))\n",
    "\n",
    "# Create encoder and decoder dictionaries\n",
    "encoder_dict = {char: i for i, char in enumerate(unique_chars)}\n",
    "decoder_dict = {i: char for i, char in enumerate(unique_chars)}\n",
    "\n",
    "# Slice the corpus into semi-redundant sequences of 20 characters\n",
    "sentence_length = 20\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoder_dict[char] for char in sentence])\n",
    "    y_data.append(encoder_dict[next_char])\n",
    "\n",
    "# Vectorize X and y\n",
    "num_chars = len(unique_chars)\n",
    "num_sentences = len(X_data)\n",
    "\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=bool)\n",
    "\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Definition\n",
    "\n",
    "Now, let's define the architecture of the LSTM-based model for text generation using Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(sentence_length, num_chars), return_sequences=True))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))  \n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "It's time to train the defined model on the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 2.8023\n",
      "Epoch 1: loss improved from inf to 2.80231, saving model to weights-01.hdf5\n",
      "1642/1642 [==============================] - 110s 57ms/step - loss: 2.8023\n",
      "Epoch 2/20\n",
      "   2/1642 [..............................] - ETA: 1:29 - loss: 2.4365"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joeyw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1642/1642 [==============================] - ETA: 0s - loss: 2.4035\n",
      "Epoch 2: loss improved from 2.80231 to 2.40355, saving model to weights-02.hdf5\n",
      "1642/1642 [==============================] - 104s 63ms/step - loss: 2.4035\n",
      "Epoch 3/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 2.2346\n",
      "Epoch 3: loss improved from 2.40355 to 2.23460, saving model to weights-03.hdf5\n",
      "1642/1642 [==============================] - 100s 61ms/step - loss: 2.2346\n",
      "Epoch 4/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 2.1230\n",
      "Epoch 4: loss improved from 2.23460 to 2.12299, saving model to weights-04.hdf5\n",
      "1642/1642 [==============================] - 104s 63ms/step - loss: 2.1230\n",
      "Epoch 5/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 2.0362\n",
      "Epoch 5: loss improved from 2.12299 to 2.03615, saving model to weights-05.hdf5\n",
      "1642/1642 [==============================] - 105s 64ms/step - loss: 2.0362\n",
      "Epoch 6/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.9666\n",
      "Epoch 6: loss improved from 2.03615 to 1.96660, saving model to weights-06.hdf5\n",
      "1642/1642 [==============================] - 107s 65ms/step - loss: 1.9666\n",
      "Epoch 7/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.9066\n",
      "Epoch 7: loss improved from 1.96660 to 1.90670, saving model to weights-07.hdf5\n",
      "1642/1642 [==============================] - 440s 268ms/step - loss: 1.9067\n",
      "Epoch 8/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.8599\n",
      "Epoch 8: loss improved from 1.90670 to 1.85981, saving model to weights-08.hdf5\n",
      "1642/1642 [==============================] - 51s 31ms/step - loss: 1.8598\n",
      "Epoch 9/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.8192\n",
      "Epoch 9: loss improved from 1.85981 to 1.81922, saving model to weights-09.hdf5\n",
      "1642/1642 [==============================] - 62s 38ms/step - loss: 1.8192\n",
      "Epoch 10/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.7826\n",
      "Epoch 10: loss improved from 1.81922 to 1.78264, saving model to weights-10.hdf5\n",
      "1642/1642 [==============================] - 69s 42ms/step - loss: 1.7826\n",
      "Epoch 11/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.7520\n",
      "Epoch 11: loss improved from 1.78264 to 1.75201, saving model to weights-11.hdf5\n",
      "1642/1642 [==============================] - 68s 41ms/step - loss: 1.7520\n",
      "Epoch 12/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.7274\n",
      "Epoch 12: loss improved from 1.75201 to 1.72745, saving model to weights-12.hdf5\n",
      "1642/1642 [==============================] - 68s 42ms/step - loss: 1.7274\n",
      "Epoch 13/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.7006\n",
      "Epoch 13: loss improved from 1.72745 to 1.70056, saving model to weights-13.hdf5\n",
      "1642/1642 [==============================] - 68s 41ms/step - loss: 1.7006\n",
      "Epoch 14/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.6768\n",
      "Epoch 14: loss improved from 1.70056 to 1.67681, saving model to weights-14.hdf5\n",
      "1642/1642 [==============================] - 68s 41ms/step - loss: 1.6768\n",
      "Epoch 15/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.6541\n",
      "Epoch 15: loss improved from 1.67681 to 1.65405, saving model to weights-15.hdf5\n",
      "1642/1642 [==============================] - 67s 41ms/step - loss: 1.6541\n",
      "Epoch 16/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.6367\n",
      "Epoch 16: loss improved from 1.65405 to 1.63667, saving model to weights-16.hdf5\n",
      "1642/1642 [==============================] - 68s 41ms/step - loss: 1.6367\n",
      "Epoch 17/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.6157\n",
      "Epoch 17: loss improved from 1.63667 to 1.61565, saving model to weights-17.hdf5\n",
      "1642/1642 [==============================] - 79s 48ms/step - loss: 1.6157\n",
      "Epoch 18/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.6051\n",
      "Epoch 18: loss improved from 1.61565 to 1.60512, saving model to weights-18.hdf5\n",
      "1642/1642 [==============================] - 74s 45ms/step - loss: 1.6051\n",
      "Epoch 19/20\n",
      "1641/1642 [============================>.] - ETA: 0s - loss: 1.5894\n",
      "Epoch 19: loss improved from 1.60512 to 1.58939, saving model to weights-19.hdf5\n",
      "1642/1642 [==============================] - 76s 47ms/step - loss: 1.5894\n",
      "Epoch 20/20\n",
      "1642/1642 [==============================] - ETA: 0s - loss: 1.5727\n",
      "Epoch 20: loss improved from 1.58939 to 1.57266, saving model to weights-20.hdf5\n",
      "1642/1642 [==============================] - 81s 50ms/step - loss: 1.5727\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "file_path = \"weights-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "history = model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "Finally, let's generate text using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " a gonna how we was the midnight I was the do If I could the star the star the day of the she the way a fire I was way I could my from the star the star on the stream The hould I can the star the time in the day I can to love is a bale it sound I want to the song that I have the she We can't gonna be a san and the sure I have it soul the sun I can the day We could my love is love is a shang Love in the star the time I have a dream To the right the sun I want to hear the can the way Oh the fan ou\n"
     ]
    }
   ],
   "source": [
    "def generate(seed_pattern):\n",
    "    # Adjust seed pattern length to match sentence_length\n",
    "    if len(seed_pattern) > sentence_length:\n",
    "        seed_pattern = seed_pattern[:sentence_length]\n",
    "    elif len(seed_pattern) < sentence_length:\n",
    "        seed_pattern = seed_pattern.ljust(sentence_length)\n",
    "\n",
    "    X = np.zeros((1, sentence_length, num_chars), dtype=bool)\n",
    "    for i, character in enumerate(seed_pattern):\n",
    "        X[0, i, encoder_dict[character]] = 1\n",
    "    \n",
    "    generated_text = \"\"\n",
    "    for i in range(500):\n",
    "        pred = model.predict(X, verbose=0)[0]\n",
    "        prediction = sample(pred, 0.3)\n",
    "        generated_text += decoder_dict[prediction]\n",
    "\n",
    "        activations = np.zeros((1, 1, num_chars), dtype=bool)\n",
    "        activations[0, 0, prediction] = 1\n",
    "        X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Generate text with a seed pattern\n",
    "seed = \"In the bard and show you on your lovelight and i can't get the mowner i'm a marion an and every mind, there's a boot\"\n",
    "generated_text = generate(seed)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered the entire process of training an LSTM-based neural network for text generation using song lyrics data. We started by preprocessing the data, defining the model architecture, training the model, and finally generating text using the trained model.\n",
    "\n",
    "By following these steps, you can apply similar techniques to train models on other text datasets and generate text in various domains.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
