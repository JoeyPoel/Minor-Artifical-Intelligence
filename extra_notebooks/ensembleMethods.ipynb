{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods in Machine Learning\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Ensemble methods are techniques that create multiple models and then combine them to produce improved results. The main idea is that by combining models, we can reduce the likelihood of errors and increase the overall accuracy and robustness of the predictions. This notebook will cover three popular ensemble methods: Bagging, Boosting, and Stacking.\n",
    "\n",
    "## 2. Problem Definition\n",
    "\n",
    "We will use the \"California Housing Prices\" dataset to demonstrate the effectiveness of ensemble methods. The goal is to predict the median house value in various districts based on features such as median income, average housing age, and total number of rooms.\n",
    "\n",
    "## 3. Ensemble Methods\n",
    "\n",
    "### 3.1 Bagging\n",
    "Bagging, or Bootstrap Aggregating, is an ensemble method that fits multiple versions of a model on different subsets of the dataset and averages their predictions.\n",
    "\n",
    "### 3.2 Boosting\n",
    "Boosting is an ensemble technique that combines weak learners to create a strong learner by training models sequentially, each new model focusing on the errors of the previous ones.\n",
    "\n",
    "### 3.3 Stacking\n",
    "Stacking involves training multiple models and then using another model (meta-learner) to combine their predictions.\n",
    "\n",
    "## 4. Evaluation Criteria\n",
    "\n",
    "The following criteria will be used to evaluate the ensemble methods:\n",
    "- **Mean Squared Error (MSE)**: A measure of the quality of the estimator.\n",
    "- **Training Time**: The time taken to train the model.\n",
    "\n",
    "## 5. Experiment Setup\n",
    "\n",
    "We will preprocess the data, split it into training and testing sets, and standardize the features. We will then train each ensemble method and evaluate their performance based on the defined criteria.\n",
    "\n",
    "## 6. Implementation and Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Load the California Housing Prices dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "data = housing.frame\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We will split the data into features (X) and target (y), then standardize the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop('MedHouseVal', axis=1)\n",
    "y = data['MedHouseVal']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "We will use the BaggingRegressor from scikit-learn to implement bagging with a base estimator of DecisionTreeRegressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging MSE: 0.2557488658168742\n",
      "Bagging Training Time: 14.415130853652954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize the base estimator\n",
    "base_estimator = DecisionTreeRegressor()\n",
    "\n",
    "# Initialize the Bagging Regressor with the base estimator\n",
    "bagging = BaggingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model and measure the training time\n",
    "start_time = time.time()\n",
    "bagging.fit(X_train, y_train)\n",
    "training_time_bagging = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
    "\n",
    "# Display results\n",
    "print(f\"Bagging MSE: {mse_bagging}\")\n",
    "print(f\"Bagging Training Time: {training_time_bagging}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "We will use the GradientBoostingRegressor from scikit-learn to implement boosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting MSE: 0.29399901242474274\n",
      "Boosting Training Time: 4.080959796905518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "boosting = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model and measure the training time\n",
    "start_time = time.time()\n",
    "boosting.fit(X_train, y_train)\n",
    "training_time_boosting = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_boosting = boosting.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_boosting = mean_squared_error(y_test, y_pred_boosting)\n",
    "\n",
    "# Display results\n",
    "print(f\"Boosting MSE: {mse_boosting}\")\n",
    "print(f\"Boosting Training Time: {training_time_boosting}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "We will use the StackingRegressor from scikit-learn to implement stacking with LinearRegression, DecisionTreeRegressor, and KNeighborsRegressor as base estimators and GradientBoostingRegressor as the meta-learner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking MSE: 0.3385051504083837\n",
      "Stacking Training Time: 4.0293190479278564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialize the Stacking Regressor\n",
    "estimators = [\n",
    "    ('lr', LinearRegression()),\n",
    "    ('dt', DecisionTreeRegressor()),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "]\n",
    "stacking = StackingRegressor(estimators=estimators, final_estimator=GradientBoostingRegressor())\n",
    "\n",
    "# Train the model and measure the training time\n",
    "start_time = time.time()\n",
    "stacking.fit(X_train, y_train)\n",
    "training_time_stacking = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_stacking = stacking.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_stacking = mean_squared_error(y_test, y_pred_stacking)\n",
    "\n",
    "# Display results\n",
    "print(f\"Stacking MSE: {mse_stacking}\")\n",
    "print(f\"Stacking Training Time: {training_time_stacking}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Based on our experiments, we can summarize the performance of each ensemble method:\n",
    "\n",
    "- **Bagging**\n",
    "  - MSE: 0.2557\n",
    "  - Training Time: 14.42 seconds\n",
    "\n",
    "- **Boosting**\n",
    "  - MSE: 0.2940\n",
    "  - Training Time: 4.08 seconds\n",
    "\n",
    "- **Stacking**\n",
    "  - MSE: 0.3385\n",
    "  - Training Time: 4.03 seconds\n",
    "\n",
    "These results indicate that the Bagging model achieved the lowest Mean Squared Error (MSE) among the three ensemble methods, with a training time of approximately 14.42 seconds. The Boosting model had a slightly higher MSE but significantly lower training time, around 4.08 seconds. The Stacking model had the highest MSE and a training time similar to the Boosting model. However please note that the dataset used is a relatively small dataset, so for this case the training time is not important but for larger datasets it might become an issue if you are using Bagging model.\n",
    "\n",
    "## 8. References\n",
    "- Scikit-learn documentation: https://scikit-learn.org/\n",
    "- California Housing Prices dataset: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
